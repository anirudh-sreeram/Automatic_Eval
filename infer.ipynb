{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob, pandas as pd, numpy as np, re\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset,concatenate_datasets, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '/mnt/atg_platform/models/now_llm_chat/v0.4.0-rc2'\n",
    "cache = 'cache_model'\n",
    "data_path = \"\"\n",
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model_base.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_output = []\n",
    "for record in tqdm(data_path):\n",
    "    inputs = inputs.split('<|end|>\\n<|user|>\\n')[0] + '<|end|>\\n<|user|>\\n' + prompt + '<|end|>\\n<|assistant|>'\n",
    "        \n",
    "    cuda_device = 'cuda:0'\n",
    "    inputs_tokenized = tokenizer(inputs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs_tokenized = {k: v.to(cuda_device) for k, v in inputs_tokenized.items()}\n",
    "        outputs = model_base.generate(\n",
    "                input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.3,\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                no_repeat_ngram_size=10,\n",
    "                repetition_penalty=1.05,\n",
    "                num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        outputs = outputs[:, inputs_tokenized[\"input_ids\"].shape[1] :]\n",
    "\n",
    "        single_result = tokenizer.batch_decode(\n",
    "            outputs.detach().cpu().numpy(), skip_special_tokens=True\n",
    "        )\n",
    "        collect_output.append(single_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
